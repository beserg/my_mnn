Great — I’ll now begin deep research to identify open-source Android projects that support local LLM inference with NNAPI or other NPU-accelerated backends, targeting both Qualcomm and MediaTek chips. I’ll focus on finding repos that can serve as a base for an offline chat app similar to PocketPal AI, optimized for LLaMA 1B–8B and compatible with GGUF, ONNX, or tflite formats.

I’ll return with working GitHub links, backend details, and a development workflow recommendation tailored for Dimensity and Snapdragon devices.


# On-Device LLM Inference with NPU Acceleration (Android)

## Introduction

Deploying large language models (LLMs) fully on-device (offline) requires optimizing for mobile hardware and leveraging specialized accelerators. Modern Android phones include NPUs (Neural Processing Units) accessible via NNAPI (Neural Networks API) or vendor SDKs (e.g. Qualcomm’s QNN, MediaTek’s NeuroPilot APU). The goal is to run a private, real-time chatbot similar to PocketPal AI but faster, by offloading heavy computation to the device’s NPU. This report surveys open-source Android projects enabling local LLM inference with hardware acceleration. We compare their support for model formats (GGUF, ONNX, TFLite), target hardware (Qualcomm Snapdragon vs. MediaTek Dimensity), and NPU backends, and then recommend a solution and development workflow for scaling from a 1B-parameter model to larger 3B–8B models.

## Candidate Open-Source Projects for On-Device LLMs

We identified three promising open-source projects that meet the criteria: **Alibaba’s MNN-LLM**, **MLC LLM (MLC Chat)**, and **BUPT/PKU’s MLLM**. All three are designed for on-device LLM inference, support multiple model architectures, and have (or plan for) hardware acceleration on mobile. Below we detail each project’s features, model support, hardware backends, NNAPI/NPU integration, and UI/usage characteristics.

### 1. **MNN-LLM (Alibaba MNN Chat)**

* **Repo**: [Alibaba MNN – *apps/Android/MnnLlmChat*](https://github.com/alibaba/MNN)
* **Description**: MNN is a high-performance deep learning framework by Alibaba. The MNN-LLM module is a runtime for deploying large language models locally across platforms (mobile, PC, IoT). It supports popular LLM families like *Qianwen, Baichuan, Zhipu, LLaMA*, etc. The Android MNN-LlmChat app is a full offline chat application (even multimodal: text, image, audio) demonstrating these models running on-device.
* **Supported Models**: Out-of-the-box, MNN-LLM supports models from small (\~0.5–3B) up to larger 7B+ (and even experimental 30B) by using quantized weights. News updates indicate support for Qwen-3 (30B), Qwen-2.5 (7B), DeepSeek 1.5B, etc. Standard transformer architectures (GPT, LLaMA variants) are supported via MNN’s converter.
* **Hardware Backends**: MNN is written in C++ with extensive optimization (ARM NEON assembly, etc.). It supports CPU (Arm v7/v8), and can utilize mobile GPUs via **OpenCL/Vulkan** for acceleration. MNN also provides a **hybrid computing** capability across CPU and GPU. Importantly, MNN has experimental support for NPUs: it can delegate to **NNAPI** (Android’s neural accelerator interface) and even has initial integration for **Qualcomm QNN**. In the support matrix, NNAPI is marked “B” (beta support) and QNN is “C/B” (incomplete, FP16 beta). This suggests MNN can target *MediaTek’s APU via NNAPI* (with some limitations) and *Snapdragon’s NPU via QNN/Hexagon* in principle.
* **NNAPI/NPU Support**: NNAPI delegation is available but not fully optimized (some ops may fall back to CPU). QNN integration was recently introduced for Qualcomm Hexagon; current release notes show QNN support in FP16 mode (to accelerate parts of model execution). MNN’s focus is still primarily CPU/GPU, but its NPU hooks provide a path to harness on-device AI cores.
* **UI or Backend**: The MNN-LLM Android Chat App includes a **full UI** for chat (text input, response display, dark mode, etc.), demonstrating interactive inference. It can be used as a reference app. Under the hood, the core inference is handled in C++ (MNN library), with a Java/Kotlin interface for the Android UI. If a UI is not needed, one could also use MNN’s C++ API or JNI to run models in a background service or CLI. MNN is Apache-2.0 licensed.

### 2. **MLC LLM (MLC Chat by MLC-AI)**

* **Repo**: [mlc-ai/mlc-llm – Android](https://github.com/mlc-ai/mlc-llm) (with an official MLC Chat app release)
* **Description**: MLC LLM is a “universal LLM deployment engine” based on machine learning compilation (TVM). It aims to optimize and run LLMs on various platforms with a single engine. MLC Chat is the Android app that lets users download and run models locally via this engine. The project emphasizes *native performance* by generating platform-specific code for each model.
* **Supported Models**: MLC provides a selection of efficient models in the 2B–8B range suitable for mobile. For example, the app features **Gemma-2B, Phi-2 2B, Mistral 7B**, and even Meta’s latest **LLaMA 3 8B**. These are typically quantized (int4/int8) to fit and run on phones. The framework can be extended to other HuggingFace or custom models by converting them to the MLC format via their Python packaging tool.
* **Hardware Backends**: On Android, MLC-LLM currently leverages **GPU acceleration** and optimized CPU kernels rather than NNAPI. Specifically, it uses *OpenCL* to run on mobile GPUs – supporting both Qualcomm’s Adreno and ARM Mali GPUs. This means it covers Snapdragon and Dimensity devices via GPU compute. (At runtime, it can also use multithreaded CPU if GPU is slow or unsupported.) Notably, MLC does **not yet use NNAPI** or Hexagon directly – as of early 2024, all inference is on CPU/GPU. For instance, MLC Chat on a Snapdragon 8 Gen 2 phone still ran on CPU in tests. The developers have stated NNAPI/NPU might be explored in the future, but it’s not in the current release.
* **NNAPI Support Status**: **Not implemented (as of 2024)**. The inference is largely CPU-bound (with GPU offload via OpenCL). As a result, token generation can be slow on older phones (e.g. \~3 tokens/sec on a Snapdragon 855+ for a 2B model). However, the app demonstrates correct functionality. Qualcomm’s AI Stack or NNAPI could dramatically improve performance when integrated. Indeed, Qualcomm claims \~8.5 tokens/sec on Snapdragon 8 Gen2’s NPU for a 7B model, so adding NNAPI/QNN would narrow this gap.
* **UI or Backend**: MLC Chat provides a **complete chat UI** on Android (model download manager, text chat interface) as a reference implementation. The MLC library itself is in C++ with Java bindings. Developers can use it as a backend (e.g. import the `ChatModule` in Java) without the provided UI, or modify the UI as needed. MLC-LLM’s design is modular and cross-platform, which is helpful for maintaining one codebase for Android and possibly other platforms (iOS, PC).

### 3. **MLLM (Mobile LLM by BUPT/PKU)**

* **Repo**: [UbiquitousLearning/mllm](https://github.com/UbiquitousLearning/mllm)
* **Description**: MLLM (not to be confused with MLC) is a research-driven project focused on *fast multimodal LLM inference on mobile devices*. Initiated by academic groups (BUPT and PKU), its goal is to achieve affordable latency for LLMs on edge devices through optimizations and NPU offloading. It’s written in plain C/C++ with no heavy dependencies, making it lightweight. An end-to-end Android demo is provided in the repo for testing.
* **Supported Models**: MLLM supports both language-only models and some vision+language models. For text LLMs, it lists compatibility with **LLaMA 2 (7B), LLaMA 3 (1B, 3B)**, Alpaca 7B, TinyLLaMA 1.1B, Mistral 7B, and various Chinese models like Qwen 0.6B–1.8B, Qwen 2.5 (1.5B), Qwen 3 (0.6B). Many of these are quantized (INT8 or even INT4) for faster inference. It even supports advanced architectures (Mixture-of-Experts up to 8×2B parameters) under a unified engine. This demonstrates a *scalable architecture* that can handle models from 0.5B to 8B+ with appropriate optimization.
* **Hardware Backends**: MLLM is optimized for **Arm CPUs (NEON) and Qualcomm NPUs**. It uses custom assembly and parallelization to maximize CPU throughput, but its standout feature is a backend for **Qualcomm’s Hexagon NPU via QNN SDK**. The team implemented an offloading strategy where the initial transformer layers (“prefill” stage) run on the Hexagon NPU, and the later inference stages run on CPU. This hybrid approach exploits the NPU for heavy computation while avoiding unsupported parts on NPU. MLLM’s QNN integration was demonstrated on Snapdragon 8 Gen3 phones, accelerating a Qwen-1.8B chat model (INT8 on NPU) with CPU fallback for the rest.
* **MediaTek Support**: **(Currently CPU-only)** – MLLM’s NPU support is specific to Qualcomm’s QNN/Hexagon. There is no built-in support for MediaTek APU as of now. On Dimensity devices, MLLM would run using its optimized CPU path (and possibly could be extended to use NNAPI if someone implements an NNAPI backend). The project’s focus on NPUs means it’s designed for extensibility: a developer could potentially integrate MediaTek’s NeuroPilot SDK or NNAPI delegate using MLLM’s backend interface. But out-of-the-box, non-Qualcomm devices must use CPU.
* **NNAPI Support**: Not directly (the project chooses QNN for now). However, since QNN is essentially Qualcomm’s proprietary analog to NNAPI, MLLM’s architecture demonstrates how to plug in an accelerator backend. The QNN backend is marked *preliminary* (active development for better performance and more models). This means future contributions could add other delegates (e.g. an NNAPI backend for generic devices).
* **UI or Backend**: MLLM provides a **command-line and programmatic interface**. The Android demo is likely a minimal UI or test harness to invoke the engine with a prompt. The emphasis is on the backend engine performance rather than a polished user interface. For integration into an app, one would wrap MLLM’s C++ engine in a JNI layer and build a custom UI (or possibly combine it with an existing chat UI like PocketPal’s). In short, consider MLLM as a powerful *backend library* to incorporate for NPU acceleration. It’s MIT licensed and designed for extensibility.

### Comparison of Key Features

The table below summarizes how these projects meet the requirements:

| Project     | Models & Formats                                                                                                                  | Hardware Backends                                                                           | NPU Acceleration                                                                                                                                                   | UI Provided                                                                                         |
| ----------- | --------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------- |
| **MNN-LLM** | Many LLMs (0.5B–30B) via MNN format or convert from ONNX/TFLite. Supports Transformers (LLaMA, etc.).                             | CPU (Armv7/v8), GPU (OpenCL, Vulkan), partial NPU (NNAPI, QNN).                             | **NNAPI:** Beta support (ops may fall back). <br> **Qualcomm QNN:** Initial FP16 support.                                                                          | Yes – Full Android chat app (text & multimodal).                                                    |
| **MLC LLM** | Small-medium LLMs (2B–8B) in proprietary packaged format. Tools to convert HF models to MLC (supports LLaMA 3, Mistral, etc.).    | CPU (multi-threaded), GPU (OpenCL on Adreno/Mali). (No vendor SDK needed, uses TVM kernels) | **NNAPI:** *Not yet* (CPU/GPU only). <br> **QNN:** No (planned via Qualcomm AI Stack in future).                                                                   | Yes – MLC Chat app with model downloader & chat UI.                                                 |
| **MLLM**    | Efficient models 0.5B–8B (INT8/INT4). Custom `.mllm` format for weights (converter provided). Supports MoE and multimodal combos. | CPU (ARM NEON, x86 AVX2), *Qualcomm Hexagon NPU* via QNN offload. (No GPU required)         | **NNAPI:** No generic support (could be added). <br> **QNN (Hexagon):** Yes, for Snapdragon 8 Gen2/Gen3 (INT8 models). Offloads part of model to NPU, rest on CPU. | Limited – basic Android demo (for research). Meant as backend engine (C++ library) for integration. |

**Sources:** Official project docs and code.

## Recommendation: Best Project for Adaptation

For building a *private offline chatbot* on Android accelerated by NPU, the most promising foundation is **Alibaba’s MNN-LLM**. MNN offers a mature, industry-tested engine with broad model support and multi-backend flexibility. Key reasons for this choice:

* **Cross-Vendor Support:** MNN can target both **Qualcomm and MediaTek**. It supports NNAPI (for MediaTek APU and any Android NPU) and has integrated Qualcomm’s QNN for Hexagon. This aligns with the goal to run on Dimensity 9300/9400 and also Snapdragon chips. MLC-LLM, by contrast, currently lacks NPU support, and MLLM is limited to Qualcomm NPU – MNN covers both with one framework.
* **Performance and Optimizations:** MNN is highly optimized (hand-tuned assembly for ARM, Winograd, etc.) and supports quantization (FP16/Int8). Even if NPU delegation is not fully working for a given model, MNN’s CPU/GPU performance is among the best, ensuring a solid baseline. MNN’s internal benchmarks show it outperforms TensorFlow Lite and others in many cases.
* **Model Format Flexibility:** You can convert models from ONNX, TensorFlow Lite, PyTorch, etc., into MNN’s format using the provided **MNN-Converter** tool. This means you could take a LLaMA-1B in ONNX or a GGUF via an intermediate, and integrate it. The conversion pipeline and optimized graph execution will simplify deploying custom models.
* **End-to-End Example:** The existence of the **MNN-LLM Chat app** as open-source is invaluable. It provides reference code for loading a model, using NNAPI/QNN delegates (if available), and a UI for interaction. We can adapt this rather than starting from scratch. For a chatbot, having a template for conversation history management, text input, etc., accelerates development.
* **Scalability:** MNN’s design can handle increasing model sizes. It already supports up to tens of billions of parameters (with quantization) in other contexts. As device NPUs become more capable (Dimensity 9400’s APU is said to handle 13B+ parameters in mixed precision), MNN can leverage that. The same app can start with a 1B model and later load a 3B or 7B model when the hardware allows, possibly by simply swapping the model file.

**Runner-Up:** *MLC LLM* is a close second for its modern approach and cross-platform compiler. If NNAPI support gets added to MLC, it could become very competitive. It already runs well on GPU and supports many of the target models. However, at present, MLC would require significant work to integrate an NPU path, whereas MNN gives us that head-start (albeit with some debugging). MLLM is cutting-edge for NPU use, but it’s more niche and might require combining with another toolkit for MediaTek.

**Conclusion:** Use **MNN-LLM** as the core runtime. We can borrow MNN’s Android app architecture to implement a LLaMA 1B chatbot now, and trust MNN’s engine to scale up to 3B and 8B models with optimizations (quantization, delegation to NPU/GPU) as hardware permits. Next, we outline a step-by-step development workflow for this approach.

## Development Workflow for NPU-Accelerated LLM Chatbot

To successfully build and scale the on-device chatbot, follow a phased approach: start simple on CPU, integrate NNAPI acceleration, ensure compatibility across chipsets, then iterate on model scaling and optimization. Below is a structured workflow:

### 1. **Initial Setup with a Small Model (CPU Baseline)**

* **Model Selection & Conversion:** Begin with a 1B-parameter LLaMA variant (or similar small model) for a proof-of-concept. Convert the model to a mobile-friendly format. For MNN, use the **MNN-Converter** to take an ONNX or PyTorch model and produce an `.mnn` file. Alternatively, convert to **FP16 or INT8** to reduce size (MNN supports FP16/Int8 quantization out of the box). Ensure the model fits in device RAM (1B \~ few hundred MB in 8-bit).
* **App Scaffolding:** Set up an Android Studio project. Include the MNN library (build from source or use a prebuilt .aar). Start with a simple activity that loads the model and runs a dummy inference. At this stage, disable any NNAPI delegation so that all computation is on CPU – this simplifies debugging and verifies the model runs correctly.
* **CPU Inference Test:** Use MNN’s C++ API via JNI or the Java wrapper (if provided) to initialize the model and run a single forward pass. For a chatbot, you will implement a loop of feeding the prompt + past tokens and getting next token logits. Confirm that you can generate text (even if slowly) on CPU. This establishes a baseline and ensures the model and tokenization are working.

### 2. **Integrate NNAPI for NPU Acceleration**

* **Enable NNAPI Delegate:** With the CPU path confirmed, turn on **NNAPI delegation** in the MNN engine. MNN may allow specifying `MNNForwardType.NNAPI` as the backend for execution, or you might need to build MNN with NNAPI support and set appropriate flags. Refer to MNN’s documentation for enabling NNAPI (noting it’s beta). If using an alternative framework (like TFLite or ONNX Runtime as a fallback), each provides an NNAPI delegate: e.g., in TFLite you call `Interpreter.Options.addDelegate(NnApiDelegate)` and ensure NNAPI is available on device.
* **Qualcomm Hexagon (QNN) Option:** For Snapdragon chips, if targeting maximum performance, consider integrating Qualcomm’s **QNN SDK** as well. In MNN this might be built-in (if compiled with QNN), or you can use Qualcomm’s libraries directly. QNN often requires the Hexagon SDK and a DSP runtime on device. MNN’s partial QNN support (FP16) could be tested on a Snapdragon 8 Gen3 device. If it yields errors or is unstable, you may choose to stick with NNAPI for Snapdragon too (since Qualcomm’s NNAPI drivers might internally use the Hexagon NPU).
* **Testing on Hardware:** Deploy the app to a **Dimensity 9300** device for NNAPI testing. Monitor the logs – NNAPI will attempt to delegate supported operations to the **APU** (MediaTek’s AI processor). If certain ops are not supported by the driver, they will execute on CPU. You can detect this by measuring performance or through NNAPI diagnostic tools. On Android 11+, you can also set `disable_nnapi_cpu` (for TFLite delegate) to force hardware use only – this helps confirm if the NPU is doing the work (if disabled, unsupported ops would cause failure instead of silently falling back to CPU).
* **Benchmark & Iterate:** Compare latency with NNAPI on vs off. Ideally, you should see throughput improvements on the NPU. For example, CPU-only on older devices was \~3 tokens/sec for small models, whereas NPUs can achieve significantly more. If improvements are minimal, profile to see if only part of the model is offloaded. It may be necessary to adjust the model (e.g. use ops that are supported by NNAPI drivers – some custom ops or unsupported activation functions might hinder delegation). At this stage, ensure **floating-point vs quantized model** compatibility with NNAPI: many NPUs prefer INT8. You might use an **INT8 quantized model** for NNAPI (and keep a higher-precision model for CPU fallback).

### 3. **Ensure Cross-Chip Compatibility (Snapdragon & Dimensity)**

* **Abstract Backend Selection:** Implement a runtime check for the device’s chip or NNAPI capabilities. For instance, on app startup, query NNAPI for available devices. Android’s NNAPI allows enumeration of accelerators (e.g. GPU, NPU by name) and selection of a preferred device. Ensure the app tries to use **“Android NNAPI”** generally – this should cover both vendors. For Qualcomm, if you have QNN integrated, you might specifically invoke it (perhaps via MNN or directly). But maintain a **fallback to NNAPI/CPU** if QNN is not applicable.
* **Qualcomm Testing:** On a Snapdragon device (e.g. Galaxy with Snapdragon 8 Gen2/Gen3), test both the NNAPI delegate and, if possible, the QNN path. The Snapdragon NNAPI driver might utilize the Hexagon NPU as well (through the system’s AI HAL). If you compiled with QNN SDK, compare performance: Qualcomm’s direct SDK might yield higher throughput for supported models. (Qualcomm has reported >8 tokens/sec on 7B models using NPU and GPU combined). However, QNN integration requires bundling Qualcomm libraries and possibly limiting app distribution (since QNN SDK might not be open-source). An alternative approach is to use **Snapdragon Neural Processing Engine (SNPE)**, but that is not open-source. Thus, sticking to NNAPI ensures portability.
* **MediaTek Testing:** On the Dimensity device, verify that NNAPI indeed utilizes the **APU**. MediaTek’s NeuroPilot generally works through NNAPI for mainstream frameworks. If available, use MediaTek’s profiling tools or ADB logs to confirm usage of the APU. You might see messages indicating the driver being used (e.g. “APU acceleration” or the name of the driver vendor).
* **Fallback Logic:** In code, handle scenarios where NPU acceleration isn’t available. For example, if NNAPI returns only a CPU reference implementation (could happen on an older or misconfigured device), log a warning and proceed with CPU or GPU execution. The app should not break; it will just run slower. Providing a settings toggle for “Enable acceleration” can be helpful for debugging – users can turn it off if it causes instability. According to Google, NNAPI will automatically fall back to CPU if an accelerator can’t handle an operation, but you can also manually restrict to CPU for comparison.

### 4. **Performance Tuning and Validation**

* **Measure Latency and Throughput:** Implement a simple benchmarking mode (or use MNN’s built-in profiler if available) to measure **ms per token** and **tokens/sec**. Compare CPU vs NNAPI on both chipsets. This quantifies the benefit of NPU offloading. For instance, if CPU gives 3 tok/s and NNAPI gives 10+ tok/s on Dimensity 9300, that’s a clear win. Ensure that for longer prompts (longer sequence prefill) the NPU significantly reduces the initial latency (as observed in research, NPU helps especially in the large matrix multiplies of the first tokens).
* **Monitor Resource Usage:** Check that using NNAPI doesn’t exhaust device memory or battery unduly. NPUs are more power-efficient for these tasks. However, loading a model that is too large can still cause memory swapping. Use Android’s `adb shell meminfo` and `systrace` to watch memory and NNAPI calls.
* **Quality Assurance:** Verify that the model outputs are correct when using NNAPI. Ideally, the results (given same seed) should match CPU inference or have only minor differences (quantization or slight floating point deviations). If there is a big discrepancy, some part of the model might not be executed or an unsupported operation might be skipped. In such cases, double-check the supported ops on NNAPI for the Android API level in question, and consider adjustments (or leave that part on CPU).
* **Edge Cases & Multithreading:** Ensure the app remains stable under continuous usage. NNAPI calls are asynchronous under the hood, but you should avoid sending a new inference request before the previous one finishes on the NPU. Implement a simple queue if needed. Also handle Android lifecycle (if app goes to background, possibly free NPU resources or pause inference).

### 5. **Android Configuration & Permissions**

* **Manifest Declarations:** Typically, NNAPI does not require special permissions (it’s part of Android OS), but if using vendor SDKs like QNN, you might need to include their libraries in the APK. No additional manifest `<uses-feature>` is strictly required for NNAPI (it’s available on all devices API 27+). However, it’s good to note the minimum API level – NNAPI was introduced in API 27 (Android 8.1) and has expanded capabilities in later versions. Target a recent API for full op coverage.
* **Neural Networks API Service:** On some devices, the NPU might be busy or off by default. Advise users to keep “AI acceleration” enabled in device settings if such an option exists. In developer options there is a toggle “Disable NNAPI” – ensure this is off when benchmarking. The app could detect if NNAPI is disabled (by an API call) and warn the user or automatically fall back to CPU if so.
* **Allowing Large Models:** If your model files are large (gigabytes), you might need to enable `android:largeHeap="true"` in the application manifest to give the VM more heap memory. Also use *File APIs* to load models from external storage or asset files efficiently (MNN can memory-map the model file). For models in external storage, request `READ_EXTERNAL_STORAGE` if applicable (for user-provided models). If downloading models in-app, ensure to do it over Wi-Fi or allow user control.
* **Optimize for Performance:** Set `android:enableOnBackInvokedCallback="true"` for smooth UI, keep the screen awake during generation (the PocketPal app has a feature to prevent sleep during inference). Use partial results streaming if your model/engine supports it, to display tokens as they are generated (for better UX). MNN and others typically generate token by token, so you can append to the UI as output comes.

### 6. **Scaling Up: 1B → 3B → 8B Models**

* **Incremental Model Upgrades:** Once the pipeline works for 1B, experiment with a 3B model (e.g. LLaMA-3B or Qwen-3B). This will increase memory usage and may slow down tokens/sec. Apply quantization aggressively: an 8-bit 3B model will be \~3GB of weights, which might fit on a device with 8–12GB RAM. For 8B models, 4-bit quantization (e.g. Q4K) may be necessary to fit \~8B parameters in memory (\~4GB). Tools like GPTQ or MNN’s quantizer can help produce int4 weights. Verify that MNN (or your engine) supports the quantized format (MNN does support Int8 and even simulated int4 via int8 weight + bitpacking).
* **NPU Compatibility for Larger Models:** As model size grows, ensure the **NPU driver can handle it**. Some NPUs have limits on model size or sequence length. You might need to use shorter prompt lengths or a smaller context for very large models if memory is constrained. Another approach is *split execution*: run some layers on NPU and then transfer to CPU/GPU. MLLM’s approach for Hexagon offloading the first 75% of layers is one example. With MNN, you may not easily configure layer-wise placement, but NNAPI might automatically partition the graph. Keep an eye on any NNAPI error indicating model too large or out of memory – if so, you can reduce batch size (i.e., process one token at a time to stream rather than full sequence at once) or limit the number of layers offloaded.
* **Testing on Dimensity 9400:** When available, test the app on a Dimensity 9400 or higher. MediaTek advertises support for up to 13B–33B parameter models on their new APU (with mixed precision INT4/INT16). This suggests that an 8B model might comfortably run on those chips with proper quantization. If using NNAPI, benefit from updates in Android that expose INT4 acceleration (Android 14 introduced some INT4 support). Make sure to use the latest NNAPI API level to leverage those improvements.
* **Profiling and Memory Optimization:** As models scale, use profiling to find bottlenecks. Perhaps the embedding layer or sampling code becomes a bottleneck on CPU even if transformer layers are accelerated. Optimize these as needed (e.g., use fast tokenization libraries or offload softmax to GPU/NPU if possible). Turn on **OpenMP** or multi-threading for any CPU components (MNN and others allow setting number of threads for CPU execution). Use pinned memory for faster CPU-GPU transfers if applicable.
* **User Experience for Large Models:** For very large models, initial prompt processing (prefill) can be lengthy. Consider implementing a progress bar or streaming partial output to keep the user engaged. Also, allow easy model swapping in the UI, so users with high-end devices can choose an 8B model, while others stick to 3B or 1B. Include a note about performance differences. Tools like PocketPal’s built-in benchmarking feature could be replicated so users know what to expect from their device.

By following this workflow, you will gradually build up a robust offline chat application. Starting with a verified CPU implementation ensures correctness, then NNAPI integration brings the speedup from on-device AI silicon. The solution will be **future-proof**: as both Qualcomm and MediaTek improve their NPUs, your app can directly take advantage via NNAPI or updated backends, and you can scale the model size to offer richer AI conversations. The end result will be a fast, private AI assistant running entirely on the user’s phone, powered by optimized LLM inference on the device’s own neural engine.

## Implementation Notes and Best Practices

* **Fallback Strategy:** Always have a path that works on **all-CPU**. If NNAPI or QNN fails or is unavailable, the app should switch to CPU or GPU. This ensures even obscure devices or future Android versions won’t break functionality – only performance might differ.
* **Continuous Updates:** Keep an eye on upstream projects like MNN and MLC for new developments. For example, if MLC LLM adds NNAPI support or if Qualcomm releases a new Hexagon SDK version, integrating those could further boost performance. MNN’s updates (e.g. adding support for new models or NPUs) should be periodically merged.
* **Community & Documentation:** Use the communities of these projects for support. MNN has a predominantly Chinese user base but active discussions; MLC has a Discord; MLLM has academic papers explaining their NPU approach. These can provide insights if you encounter issues like unsupported ops or quantization errors.
* **Model Fine-tuning:** Consider fine-tuning your chosen LLM on a smaller context or quantization-aware training if possible, to maximize its quality under int8/int4 quantization. A model specifically tuned for 4-bit might outperform a generic quantized model.
* **Privacy and Offline Assurance:** Emphasize in the app (as PocketPal does) that no data leaves the device. This is a key user benefit. Ensure the app does not request internet permission (unless for model download) to give users confidence everything is local.
* **Realistic Expectations:** Finally, communicate to end-users that while on-device AI is improving, a 1B–8B model is not GPT-4. It’s suitable for casual chat or basic assistance, but not all answers will be perfect. However, the convenience and privacy of offline usage is the trade-off. As hardware and models evolve, the experience will continually improve.

With the above approach, you will create a cutting-edge offline AI chat app that fully harnesses the **device’s NPU** for acceleration. It will work across Qualcomm and MediaTek devices, scale with newer chips, and maintain user privacy by never needing to send data to the cloud. Good luck with your development!

**Sources:** Key insights were drawn from official project documentation and benchmarks, such as MNN’s support matrix, MLC’s compiler approach, MLLM’s NPU offloading design, and reports of mobile LLM performance. These sources substantiate the feasibility and guide the implementation of an NPU-accelerated LLM on Android.
